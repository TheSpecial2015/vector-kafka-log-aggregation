[sources.log_file]
type = "file"
include = ["/logs/output.log"]
read_from = "beginning"
glob_minimum_cooldown_ms = 10  # Poll every 10ms (was polling_interval_secs)
max_read_bytes = 2048       # Read up to 2KB at a time        

[transforms.parse_logs]
type = "remap"
inputs = ["log_file"]
source = '''
parsed = parse_grok!(.message, "%{TIMESTAMP_ISO8601:timestamp} \\[%{WORD:level}\\] %{GREEDYDATA:message}")

if parsed.level != "warning" && parsed.level != "error" {
  abort
}

.timestamp = parse_timestamp!(parsed.timestamp, "%Y-%m-%dT%H:%M:%SZ")
.level = parsed.level
.log_message = parsed.message

.source = "log-generator"
'''

[sinks.console]
type = "console"
inputs = ["parse_logs"]
encoding.codec = "json"

[sinks.kafka_out]
type = "kafka"
inputs = ["parse_logs"]
bootstrap_servers = "kafka:9092"
topic = "logs"
encoding.codec = "json"
compression = "snappy"
batch.max_events = 1          # Send each event instantly
batch.timeout_secs = 0.01     # Flush every 10ms if not sent

# Add the following section to enable the HTTP API
# Thi section is optional but certainly useful
[api]
enabled = true
address = "0.0.0.0:8686"